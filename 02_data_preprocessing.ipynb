{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Preprocessing and Cleaning\n",
        "\n",
        "This notebook takes the raw Lending Club dataset and performs a series of preprocessing and cleaning steps. The goal is to create a clean, model-ready dataset by:\n",
        "\n",
        "1.  Filtering for relevant loan outcomes.\n",
        "2.  Creating a binary target variable for default prediction.\n",
        "3.  Removing features that would cause data leakage.\n",
        "4.  Handling missing values.\n",
        "5.  Cleaning and transforming data types.\n",
        "6.  Engineering a new feature for credit history length.\n",
        "7.  Dropping unnecessary identifier and text columns.\n",
        "\n",
        "The final cleaned DataFrame will be saved to a new CSV file."
      ],
      "metadata": {
        "id": "TolCq1Bfn2P0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AdKPfZAnwA2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration\n",
        "\n",
        "Define the input file (the raw dataset) and the output file for our cleaned data."
      ],
      "metadata": {
        "id": "PEOMkCRNn8AX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "INPUT_FILE = 'lc_accepted_loans_full_2007to2018.csv'\n",
        "OUTPUT_FILE = 'lc_loans_cleaned.csv'"
      ],
      "metadata": {
        "id": "f_w-7Zdan-Mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Load Raw Dataset\n",
        "\n",
        "Load the raw data by uploading the `lc_accepted_loans_full_2007to2018.csv` file."
      ],
      "metadata": {
        "id": "QyhrrLVToAyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    df = pd.read_csv(INPUT_FILE, low_memory=False)\n",
        "    print(\"1. Loading raw dataset...\")\n",
        "    print(f\"   - Initial shape: {df.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{INPUT_FILE}' was not found.\")\n",
        "    print(\"Please make sure you've uploaded the file to this Colab session.\")"
      ],
      "metadata": {
        "id": "LBpqBdvAoDJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Filter and Encode the Target Variable\n",
        "\n",
        "For our predictive model, we only need loans that have reached a final outcome. We will filter the dataset to keep only loans that are **'Fully Paid'** (good loans) or **'Charged Off'** (bad loans).\n",
        "\n",
        "We will then create a new binary `target` column where `1` represents a 'Charged Off' loan and `0` represents a 'Fully Paid' loan."
      ],
      "metadata": {
        "id": "s2ecfFn1oFwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n2. Handling Target Variable ('loan_status')...\")\n",
        "\n",
        "# Keep loans with a terminal status only (\"Charged Off\", \"Fully Paid\")\n",
        "terminal_statuses = ['Fully Paid', 'Charged Off']\n",
        "df = df[df['loan_status'].isin(terminal_statuses)].copy()\n",
        "print(f\"   - Shape after keeping terminal statuses: {df.shape}\")\n",
        "\n",
        "# Create a binary target variable\n",
        "df['target'] = df['loan_status'].apply(lambda x: 1 if x == 'Charged Off' else 0)\n",
        "df = df.drop(columns=['loan_status'])\n",
        "print(\"   - Created binary 'target' column and dropped original 'loan_status'.\")"
      ],
      "metadata": {
        "id": "3sEDRNOsoIJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Remove Columns with Data Leakage\n",
        "\n",
        "To build a realistic model, we must remove any features that contain information that would not have been available at the time of a loan application. These are often called \"performance variables\" and their inclusion would cause data leakage, leading to an overly optimistic model."
      ],
      "metadata": {
        "id": "leTPF1hloKTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n3. Removing columns that cause data leakage (performance variables)...\")\n",
        "# These columns contain information about borrower behavior after the loan was funded.\n",
        "leaky_cols = [\n",
        "    'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv',\n",
        "    'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee',\n",
        "    'recoveries', 'collection_recovery_fee', 'last_pymnt_d',\n",
        "    'last_pymnt_amnt', 'next_pymnt_d', 'last_credit_pull_d',\n",
        "    'last_fico_range_high', 'last_fico_range_low'\n",
        "]\n",
        "df = df.drop(columns=leaky_cols, errors='ignore')\n",
        "print(f\"   - Dropped {len(leaky_cols)} leaky columns.\")\n",
        "print(f\"   - Shape after dropping leaky columns: {df.shape}\")"
      ],
      "metadata": {
        "id": "3HnGkfwSoMzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Handle Missing Values\n",
        "\n",
        "We will handle missing data using a two-step strategy:\n",
        "1.  **Drop Sparse Columns:** First, we remove any column that is missing more than 30% of its values.\n",
        "2.  **Drop Rows with NaNs:** After removing the sparsest columns, we drop any remaining rows that still contain missing values."
      ],
      "metadata": {
        "id": "Ant9sTZmoOwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n4. Handling missing values...\")\n",
        "\n",
        "# Drop columns with more than 30% missing data\n",
        "missing_threshold = 0.30\n",
        "missing_fractions = df.isnull().mean()\n",
        "cols_to_drop_by_missing = missing_fractions[missing_fractions > missing_threshold].index\n",
        "df = df.drop(columns=cols_to_drop_by_missing)\n",
        "print(f\"   - Dropped {len(cols_to_drop_by_missing)} columns with >{missing_threshold*100}% missing values.\")\n",
        "print(f\"   - Shape after dropping sparse columns: {df.shape}\")\n",
        "\n",
        "# Drop any remaining rows with missing data\n",
        "rows_before_drop = len(df)\n",
        "df = df.dropna().copy()\n",
        "rows_after_drop = len(df)\n",
        "print(f\"   - Dropped {rows_before_drop - rows_after_drop} rows with remaining missing values.\")\n",
        "print(f\"   - Shape after dropping rows with NaNs: {df.shape}\")"
      ],
      "metadata": {
        "id": "NnHUk8eLoQtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Clean Data Types and Feature Engineering\n",
        "\n",
        "We keep some key features to make them usable in machine learning. This includes converting text-based numbers into integers and engineering a new feature for the length of the borrower's credit history."
      ],
      "metadata": {
        "id": "Cd19yf21oTc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n5. Cleaning data types and engineering features...\")\n",
        "\n",
        "# Clean 'term' column ('36 months' -> 36)\n",
        "df['term'] = df['term'].str.extract(r'(\\d+)').astype(int)\n",
        "\n",
        "# Clean 'emp_length' column ('10+ years' -> 10, '< 1 year' -> 0)\n",
        "def clean_emp_length(length_str):\n",
        "    if pd.isna(length_str): return None\n",
        "    if length_str == '10+ years': return 10\n",
        "    if length_str == '< 1 year': return 0\n",
        "    match = re.search(r'\\d+', str(length_str))\n",
        "    if match:\n",
        "        return int(match.group())\n",
        "    return None\n",
        "\n",
        "df['emp_length'] = df['emp_length'].apply(clean_emp_length)\n",
        "\n",
        "# errors='coerce' will force any bad dates into NaT (Not a Time)\n",
        "df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'], errors='coerce')\n",
        "df['issue_d'] = pd.to_datetime(df['issue_d'], errors='coerce')\n",
        "\n",
        "# Drop any rows that failed the date conversion (now have NaT)\n",
        "# or failed the emp_length conversion\n",
        "rows_before = len(df)\n",
        "df.dropna(subset=['earliest_cr_line', 'issue_d', 'emp_length'], inplace=True)\n",
        "rows_after = len(df)\n",
        "print(f\"   - Dropped {rows_before - rows_after} rows due to bad date or emp_length values.\")\n",
        "\n",
        "# Covert the columns into datetime objects\n",
        "# to calculate the credit history length feature\n",
        "df['credit_history_length_days'] = (df['issue_d'] - df['earliest_cr_line']).dt.days\n",
        "\n",
        "print(\"   - Cleaned 'term' and 'emp_length' columns.\")\n",
        "print(\"   - Created 'credit_history_length_days' feature.\")"
      ],
      "metadata": {
        "id": "JfpYR2vJtDqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Drop Other Unnecessary Columns\n",
        "\n",
        "Finally, we remove columns that are not useful for a predictive model, such as unique identifiers, URLs, and free-text fields."
      ],
      "metadata": {
        "id": "W-shZ5gmoYQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n6. Dropping other unnecessary columns (IDs, free text, etc.)...\")\n",
        "unnecessary_cols = [\n",
        "    'id', 'member_id', 'url', 'title', 'emp_title', 'zip_code',\n",
        "    'addr_state', 'issue_d', 'earliest_cr_line' # Drop original date columns\n",
        "]\n",
        "df = df.drop(columns=unnecessary_cols, errors='ignore')\n",
        "print(f\"   - Dropped {len(unnecessary_cols)} unnecessary columns.\")"
      ],
      "metadata": {
        "id": "tdpssB6moaL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Step: Save the Cleaned Dataset\n",
        "\n",
        "The preprocessing is complete. We will now save the cleaned and transformed DataFrame to a new CSV file. We'll also display a summary of the final dataset to confirm the data types and column count."
      ],
      "metadata": {
        "id": "5WtxE6g1ochJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n7. Saving cleaned data...\")\n",
        "\n",
        "# Save the final cleaned DataFrame\n",
        "df.to_csv(OUTPUT_FILE, index=False)\n",
        "\n",
        "print(f\"   - Final shape of cleaned data: {df.shape}\")\n",
        "print(f\"   - Success! Cleaned dataset saved to '{OUTPUT_FILE}'\")\n",
        "print(\"\\n--- Data Preprocessing Complete ---\")\n",
        "\n",
        "# Display a summary of the final DataFrame\n",
        "print(\"\\nFinal DataFrame Info:\")\n",
        "df.info()"
      ],
      "metadata": {
        "id": "12VFmL1Boeho"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}