{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 6.1. Model Training: Logistic Regression\n",
        "\n",
        "This notebook uses the preprocessed, scaled, and balanced data from our previous step to train our first machine learning model: **Logistic Regression**.\n",
        "\n",
        "The process will be:\n",
        "1.  **Load** the `X_train`, `y_train`, `X_test`, and `y_test` files.\n",
        "2.  **Initialize** the Logistic Regression model.\n",
        "3.  **Train** the model on the `_train` data.\n",
        "4.  **Evaluate** the model's performance on the unseen `_test` data to see how well it learned to predict loan defaults."
      ],
      "metadata": {
        "id": "gcmkG71FvyjC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCLQi6lFu2Kj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    ConfusionMatrixDisplay\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Load the Prepared Datasets\n",
        "\n",
        "First, we load our four pre-processed data files `X_train.csv`, `y_train.csv`, `X_test.csv`, and `y_test.csv`."
      ],
      "metadata": {
        "id": "QpL7IgHWv14D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    X_train = pd.read_csv('X_train.csv')\n",
        "    y_train = pd.read_csv('y_train.csv')\n",
        "    X_test = pd.read_csv('X_test.csv')\n",
        "    y_test = pd.read_csv('y_test.csv')\n",
        "\n",
        "    print(\"All training and testing data loaded successfully.\")\n",
        "    print(f\"X_train shape: {X_train.shape}\")\n",
        "    print(f\"X_test shape: {X_test.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Could not find all four .csv files.\")\n",
        "    print(\"Please upload X_train, y_train, X_test, and y_test to this Colab session.\")"
      ],
      "metadata": {
        "id": "4l_91fTPv33J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Initialize and Train the Model\n",
        "\n",
        "Next, we will create an instance of the `LogisticRegression` model and train it by calling the `.fit()` method. The model \"learns\" by finding the best patterns in `X_train` that predict the answers in `y_train`."
      ],
      "metadata": {
        "id": "-qi894QHv6MW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "print(\"Training the Logistic Regression model...\")\n",
        "\n",
        "# Check data types in X_train\n",
        "print(\"Checking data types in X_train:\")\n",
        "print(X_train.dtypes)\n",
        "\n",
        "# Identify and remove non-numeric columns if any\n",
        "non_numeric_cols = X_train.select_dtypes(exclude=[np.number]).columns\n",
        "if not non_numeric_cols.empty:\n",
        "    print(f\"\\nRemoving non-numeric columns from X_train: {list(non_numeric_cols)}\")\n",
        "    X_train = X_train.drop(columns=non_numeric_cols)\n",
        "    # Apply the same changes to X_test\n",
        "    X_test = X_test.drop(columns=non_numeric_cols)\n",
        "    print(\"Non-numeric columns removed.\")\n",
        "else:\n",
        "    print(\"\\nNo non-numeric columns found in X_train.\")\n",
        "\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "print(\"Model training complete.\")"
      ],
      "metadata": {
        "id": "7sSJhZaNv8EJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Make Predictions on the Test Data\n",
        "\n",
        "Now that the model is trained, we'll use it to make predictions on the unseen `X_test` data."
      ],
      "metadata": {
        "id": "JNmVCbnov-0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The model will look at every row in X_test and predict 0 (Fully Paid) or 1 (Charged Off)\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "print(\"Predictions made on the test set.\")"
      ],
      "metadata": {
        "id": "JubRjM0TwAaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Evaluate Model Performance - The \"Report Card\"\n",
        "\n",
        "We then compare the model's `predictions` to the *true answers* in `y_test`.\n",
        "\n",
        "###1. Accuracy\n",
        "First, we'll look at **Accuracy**, which is the percentage of the total predictions the model got right."
      ],
      "metadata": {
        "id": "FOBMKobTwCLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the model's predictions to the actual answers\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "sFwMgrXjwFJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Classification Report (Precision, Recall, and F1-Score)\n",
        "\n",
        "Accuracy can be misleading, especially in risk. We need to know what kind of correct and incorrect predictions it's making. The **Classification Report** gives us these crucial metrics.\n",
        "\n",
        "* **Precision (Class 1):** Of all the loans the model *predicted* would default, what percentage actually did? **(High precision = The model is trustworthy when it flags a loan as bad).**\n",
        "* **Recall (Class 1):** Of all the loans that *actually* defaulted, what percentage did the model successfully catch? **(High recall = The model is good at finding most of the bad loans).**\n",
        "* **F1-Score:** The balanced average of Precision and Recall."
      ],
      "metadata": {
        "id": "mopz5hFjwIhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the full classification report\n",
        "report = classification_report(y_test, predictions, target_names=['Fully Paid (0)', 'Charged Off (1)'])\n",
        "\n",
        "print(\"--- Classification Report ---\")\n",
        "print(report)"
      ],
      "metadata": {
        "id": "2SXgwaD1wJev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. The Confusion Matrix\n",
        "\n",
        "The **Confusion Matrix** is a visual breakdown of all predictions. It shows us the *four types* of predictions the model made.\n",
        "\n",
        "* **True Negatives (Top-Left):** Correctly predicted 'Fully Paid'.\n",
        "* **True Positives (Bottom-Right):** Correctly predicted 'Charged Off'.\n",
        "* **False Positives (Top-Right):** *Incorrectly* labeled a good loan as 'Charged Off'. (\"safe error,\" - the model denied a good applicant).\n",
        "* **False Negatives (Bottom-Left):** *Incorrectly* labeled a bad loan as 'Fully Paid'. (\"costly error!\" - the model approved a loan that will default)."
      ],
      "metadata": {
        "id": "xbDm-4S0wLMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Confusion Matrix ---\")\n",
        "\n",
        "# Generate and display the Confusion Matrix\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ConfusionMatrixDisplay.from_predictions(\n",
        "    y_test,\n",
        "    predictions,\n",
        "    ax=ax,\n",
        "    display_labels=['Fully Paid', 'Charged Off'],\n",
        "    cmap='Blues'\n",
        ")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rkLOW17AwNa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Interpret the Model (Feature Importance)\n",
        "\n",
        "Finally, we look at features which the Logistic Regression model found to be the most predictive by pulling the \"coefficients\" (or weights) it assigned to each feature.\n",
        "\n",
        "* **Large Positive Coefficient:** This feature strongly predicts a **Default (1)**.\n",
        "* **Large Negative Coefficient:** This feature strongly predicts a **Full Repayment (0)**."
      ],
      "metadata": {
        "id": "X32lmRO0wPi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- Define the Output Directory ---\n",
        "OUTPUT_DIR = \"logistic_regression_results\"\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Get the coefficients from the trained model\n",
        "coefficients = model.coef_[0]\n",
        "\n",
        "# Get the feature names from the X_train columns\n",
        "feature_names = X_train.columns\n",
        "\n",
        "# Create a DataFrame to see them clearly\n",
        "importance_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
        "\n",
        "# Sort by coefficient to see the most influential features\n",
        "importance_df = importance_df.sort_values(by='Coefficient', ascending=False)\n",
        "\n",
        "print(\"--- Top 10 Features Predicting DEFAULT (High Risk) ---\")\n",
        "display(importance_df.head(10))\n",
        "\n",
        "print(\"\\n--- Top 10 Features Predicting FULL REPAYMENT (Low Risk) ---\")\n",
        "display(importance_df.tail(10).sort_values(by='Coefficient', ascending=True))\n",
        "\n",
        "# --- Plot the Top 10 Features Predicting DEFAULT (High Risk) ---\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.barplot(\n",
        "    x='Coefficient',\n",
        "    y='Feature',\n",
        "    data=importance_df.head(10)\n",
        ")\n",
        "plt.title('Top 10 Features Predicting Default (Logistic Regression)')\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"lr_feature_importance_plot.png\"))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HfVYZ6q4wRM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZOlN-jd1pzQ_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}