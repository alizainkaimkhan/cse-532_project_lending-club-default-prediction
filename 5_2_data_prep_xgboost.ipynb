{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 5.2. Data Preparation for XGBoost\n",
        "\n",
        "This notebook prepares the `lc_loans_master_sample.csv` file for use with XGBoost.\n",
        "\n",
        "This process varies from the one for Logistic Regression:\n",
        "1.  **No Feature Scaling:** Tree models are not sensitive to the scale of features, so we do not need to use `StandardScaler`.\n",
        "2.  **Ordinal Encoding:** We will convert all categorical columns into simple integer labels (e.g., A=0, B=1, C=2). This is more efficient for tree models than one-hot encoding.\n",
        "3.  **Feature Selection:** We will drop the same redundant and unneeded columns as before."
      ],
      "metadata": {
        "id": "OCUMOfuDseMJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGt05rXNsFAE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration\n",
        "\n",
        "Define the input file (our master sample) and the new output file names. We'll add `_tree` to distinguish them from the Logistic Regression files."
      ],
      "metadata": {
        "id": "aeVjFntCsyn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "INPUT_FILE = 'lc_loans_master_sample.csv'"
      ],
      "metadata": {
        "id": "V3TnJfJQs0-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Load and Prepare Data\n",
        "\n",
        "Load the master sample, create our `loan_to_income_ratio` feature, and drop all redundant or unneeded columns."
      ],
      "metadata": {
        "id": "bqO27kP2s2fl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the master sample dataset\n",
        "try:\n",
        "    df = pd.read_csv(INPUT_FILE)\n",
        "    print(f\"Loaded master sample with shape: {df.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{INPUT_FILE}' was not found.\")\n",
        "    print(\"Please upload the file to this Colab session.\")\n",
        "\n",
        "# 1. Feature Engineering\n",
        "df['loan_to_income_ratio'] = df['loan_amnt'] / (df['annual_inc'] + 1)\n",
        "print(\"Created 'loan_to_income_ratio' feature.\")\n",
        "\n",
        "# 2. Feature Selection\n",
        "# Drop all redundant, leaky, or unneeded text columns\n",
        "features_to_drop = [\n",
        "    # Redundant with sub_grade\n",
        "    'grade',\n",
        "    'int_rate',\n",
        "\n",
        "    # Redundant with loan_amnt and term\n",
        "    'installment',\n",
        "\n",
        "    # Unneeded text columns\n",
        "    'pymnt_plan',\n",
        "    'initial_list_status',\n",
        "    'application_type',\n",
        "    'hardship_flag',\n",
        "    'disbursement_method',\n",
        "    'debt_settlement_flag'\n",
        "]\n",
        "df = df.drop(columns=features_to_drop, errors='ignore')\n",
        "print(f\"Dropped {len(features_to_drop)} redundant/unneeded features.\")"
      ],
      "metadata": {
        "id": "oYAIFL11s5vZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Define Features (X) and Target (y)\n",
        "\n",
        "Separate the data into `X` (features) and `y` (target)."
      ],
      "metadata": {
        "id": "BYMpbQ9zs_QR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['target']\n",
        "X = df.drop(columns='target')\n",
        "\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")"
      ],
      "metadata": {
        "id": "7u-UkrRPtBVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Train-Test Split\n",
        "\n",
        "Split the data into training and testing sets, ensuring the 50/50 balance is preserved using `stratify=y`."
      ],
      "metadata": {
        "id": "FouXMwgotDu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,     # 20% for testing\n",
        "    random_state=42,\n",
        "    stratify=y         # Keep the 50/50 balance\n",
        ")\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "XSluwcZAtIhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Feature Encoding (Ordinal)\n",
        "\n",
        "We will find all remaining text (`object`) columns and convert them to ranked integers using `OrdinalEncoder`."
      ],
      "metadata": {
        "id": "Q-P9lNMYtLM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Starting Ordinal Encoding ---\")\n",
        "\n",
        "# Automatically find all categorical (text) columns\n",
        "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
        "\n",
        "print(f\"Found {len(categorical_cols)} categorical columns to encode: {list(categorical_cols)}\")\n",
        "\n",
        "# Initialize the OrdinalEncoder\n",
        "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "\n",
        "# Fit the encoder on the training data\n",
        "encoder.fit(X_train[categorical_cols])\n",
        "\n",
        "# Transform both the training and test data\n",
        "X_train[categorical_cols] = encoder.transform(X_train[categorical_cols])\n",
        "X_test[categorical_cols] = encoder.transform(X_test[categorical_cols])\n",
        "\n",
        "print(\"Ordinal encoding complete.\")\n",
        "print(\"\\n--- Preprocessing for Tree Models Complete ---\")"
      ],
      "metadata": {
        "id": "pQU4DmTotOMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Save the Prepared Datasets\n",
        "\n",
        "Save the new, tree-ready files."
      ],
      "metadata": {
        "id": "VQ9amVi9tQTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the final, processed data\n",
        "X_train.to_csv('X_train_tree.csv', index=False)\n",
        "y_train.to_csv('y_train_tree.csv', index=False)\n",
        "X_test.to_csv('X_test_tree.csv', index=False)\n",
        "y_test.to_csv('y_test_tree.csv', index=False)\n",
        "\n",
        "print(\"Saved X_train_tree, y_train_tree, X_test_tree, and y_test_tree to CSV files.\")\n",
        "\n",
        "print(\"\\n--- Final X_train_tree head (note: no scaling): ---\")\n",
        "# Display a sample of the final data. All values should be numeric.\n",
        "display(X_train.head())"
      ],
      "metadata": {
        "id": "VW6fIoeTtSZT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}