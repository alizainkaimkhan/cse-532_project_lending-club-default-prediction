{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 5.3. Data Preparation for Categorical Boost (CatBoost)\n",
        "\n",
        "This notebook prepares a new version of our dataset for the CatBoost classifier.\n",
        "\n",
        "In our previous modeling, we removed \"redundant\" features (like `grade` and `int_rate`) to prevent multicollinearity, which breaks Logistic Regression.\n",
        "\n",
        "However, the literature (Ko et al., 2022) suggests that **tree-based models** often perform better when they have access to *all* variations of a feature.\n",
        "* `int_rate` gives the tree a precise, continuous split point (e.g., > 12.5%).\n",
        "* `grade` gives the tree a broad, categorical bucket (e.g., \"is Grade D?\").\n",
        "\n",
        "By keeping these highly correlated features, we allow the model to find both coarse and fine-grained patterns.\n",
        "\n",
        "**Goal:** Create a new set of training and testing datasets that include `int_rate`, `grade`, `sub_grade`, and `installment`."
      ],
      "metadata": {
        "id": "3dnq-HArBWt0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIzvMGhABH2Y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration\n",
        "We define the input file (our master sample) and a new suffix `_tree_full` for the output files."
      ],
      "metadata": {
        "id": "pE36J9kY6ETy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "INPUT_FILE = 'lc_loans_master_sample.csv'"
      ],
      "metadata": {
        "id": "-qBsn-rG6IQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Load and Feature Engineering\n",
        "\n",
        "We load the master sample and re-create our custom `loan_to_income_ratio` feature."
      ],
      "metadata": {
        "id": "728qzTav6I-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the master sample dataset\n",
        "try:\n",
        "    df = pd.read_csv(INPUT_FILE)\n",
        "    print(f\"Loaded master sample with shape: {df.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{INPUT_FILE}' was not found.\")\n",
        "    print(\"Please upload the file to this Colab session.\")\n",
        "\n",
        "# Feature Engineering\n",
        "df['loan_to_income_ratio'] = df['loan_amnt'] / (df['annual_inc'] + 1)\n",
        "print(\"Created 'loan_to_income_ratio' feature.\")"
      ],
      "metadata": {
        "id": "8nzo68wH6MpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Revised Feature Selection (Keeping Redundant Features)\n",
        "\n",
        "**Crucial Change:** Unlike our previous preparation, we are **NOT** dropping `grade`, `int_rate`, or `installment`. We are only dropping the administrative text columns that provide no predictive value or cause data leakage."
      ],
      "metadata": {
        "id": "fEKeEsoX6PTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Features to drop\n",
        "features_to_drop = [\n",
        "    'pymnt_plan',\n",
        "    'initial_list_status',\n",
        "    'application_type',\n",
        "    'hardship_flag',\n",
        "    'disbursement_method',\n",
        "    'debt_settlement_flag'\n",
        "]\n",
        "\n",
        "df = df.drop(columns=features_to_drop, errors='ignore')\n",
        "print(f\"Dropped {len(features_to_drop)} administrative features.\")\n",
        "print(f\"Current columns included: {len(df.columns)}\")"
      ],
      "metadata": {
        "id": "h8rixjNB6RxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Train-Test Split\n",
        "\n",
        "We split the data into training and testing sets, preserving the 50/50 class balance."
      ],
      "metadata": {
        "id": "XaCdYJSU6Tm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['target']\n",
        "X = df.drop(columns='target')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,     # 20% for testing\n",
        "    random_state=42,\n",
        "    stratify=y         # Keep the 50/50 balance\n",
        ")\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "Wxm-qGFO6Vo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Ordinal Encoding (All Categorical Features)\n",
        "\n",
        "Since tree models handle integers well, we will use `OrdinalEncoder` to convert *all* remaining text columns (including `grade`, `sub_grade`, `home_ownership`, etc.) into numbers (0, 1, 2...).\n",
        "\n",
        "Note: `OrdinalEncoder` automatically sorts alphabetically, so 'A' becomes 0, 'B' becomes 1, etc., which perfectly preserves the rank for `grade` and `sub_grade`."
      ],
      "metadata": {
        "id": "E5odOUHF6XI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Starting Ordinal Encoding ---\")\n",
        "\n",
        "# Automatically find all categorical (text) columns\n",
        "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
        "\n",
        "print(f\"Encoding the following {len(categorical_cols)} columns: {list(categorical_cols)}\")\n",
        "\n",
        "# Initialize the OrdinalEncoder\n",
        "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "\n",
        "# Fit on training data\n",
        "encoder.fit(X_train[categorical_cols])\n",
        "\n",
        "# Transform both sets\n",
        "X_train[categorical_cols] = encoder.transform(X_train[categorical_cols])\n",
        "X_test[categorical_cols] = encoder.transform(X_test[categorical_cols])\n",
        "\n",
        "print(\"Ordinal encoding complete.\")"
      ],
      "metadata": {
        "id": "9PHRIHx66YrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Save the \"Full\" Datasets\n",
        "\n",
        "We save these new files with the `_tree_full` suffix. These will be the input for our CatBoost, Tuned XGBoost, and Stacking models."
      ],
      "metadata": {
        "id": "dQlJtR1r6aNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the final, processed data\n",
        "X_train.to_csv('X_train_tree_full.csv', index=False)\n",
        "y_train.to_csv('y_train_tree_full.csv', index=False)\n",
        "X_test.to_csv('X_test_tree_full.csv', index=False)\n",
        "y_test.to_csv('y_test_tree_full.csv', index=False)\n",
        "\n",
        "print(\"Saved X_train_tree_full, y_train_tree_full, X_test_tree_full, and y_test_tree_full.\")\n",
        "\n",
        "# Display a sample to verify grade/int_rate are present\n",
        "print(\"\\n--- Verification: Checking for 'grade' and 'int_rate' ---\")\n",
        "cols_to_check = ['grade', 'sub_grade', 'int_rate', 'installment']\n",
        "display(X_train[cols_to_check].head())"
      ],
      "metadata": {
        "id": "4zNGw2Ju6bjE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}