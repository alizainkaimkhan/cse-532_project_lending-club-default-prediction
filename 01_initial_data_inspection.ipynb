{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Initial Inspection of the Raw Lending Club Dataset\n",
        "\n",
        "This notebook performs a comprehensive initial inspection of the raw Lending Club accepted loans dataset (`lc_accepted_loans_full_2007to2018.csv`).\n",
        "\n",
        "The primary goals are to:\n",
        "- Understand the dataset's structure, size, and data types.\n",
        "- Analyze the distribution of the target variable, `loan_status`.\n",
        "- Identify columns with missing data.\n",
        "- Get a high-level statistical summary of the numerical features.\n",
        "- Create a small, stratified sample for quick visual checks."
      ],
      "metadata": {
        "id": "0JPL9Acsjli0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re"
      ],
      "metadata": {
        "id": "VJrDmEGCjcH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration\n",
        "\n",
        "Here, we define the file paths and key parameters for the analysis."
      ],
      "metadata": {
        "id": "TgX8epOcji_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "FILE_TO_ANALYZE = 'lc_accepted_loans_full_2007to2018.csv'\n",
        "SAMPLE_OUTPUT_FILE = 'lc_loans_stratified_sample.csv'\n",
        "TOTAL_SAMPLES = 100"
      ],
      "metadata": {
        "id": "5CAaLeohjyi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Load the Dataset\n",
        "\n",
        "First, we load the raw dataset from the specified CSV file. We'll then display the shape of the data (rows, columns) and view the first few rows to get a feel for the content."
      ],
      "metadata": {
        "id": "TrE_KJYCj2ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload 'lc_accepted_loans_full_2007to2018.csv' file\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(FILE_TO_ANALYZE, low_memory=False, on_bad_lines='skip')\n",
        "    print(\"Dataset loaded successfully!\")\n",
        "    print(f\"Shape of the dataset (rows, columns): {df.shape}\")\n",
        "    display(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{FILE_TO_ANALYZE}' was not found.\")\n",
        "    print(\"Please make sure you've uploaded the file to this Colab session.\")"
      ],
      "metadata": {
        "id": "PooArti-j5vA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Target Variable Analysis (`loan_status`)\n",
        "\n",
        "Understanding the distribution of our target variable is crucial. We will count the occurrences of each loan status and calculate its percentage of the total."
      ],
      "metadata": {
        "id": "Bp1UIst6j80R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_col = 'loan_status'\n",
        "\n",
        "if target_col in df.columns:\n",
        "    print(f\"--- Analyzing Target Variable: {target_col} --- \")\n",
        "    counts = df[target_col].value_counts()\n",
        "    percentages = df[target_col].value_counts(normalize=True) * 100\n",
        "\n",
        "    summary_df = pd.DataFrame({'Count': counts, 'Percentage': percentages.map('{:.2f}%'.format)})\n",
        "    summary_df = summary_df.reset_index().rename(columns={'index': target_col})\n",
        "\n",
        "    print(\"Distribution of the target variable:\")\n",
        "    print(summary_df.to_string(index=False))\n",
        "else:\n",
        "    print(f\"Warning: Target variable '{target_col}' not found.\")"
      ],
      "metadata": {
        "id": "NZWKdZOMkCGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Inferred Statistical Variable Types\n",
        "\n",
        "Pandas' default data types (`Dtype`) are technical. This step infers the *statistical purpose* of each column (e.g., Numerical, Categorical, Identifier) based on its data type and the number of unique values. This gives us a more intuitive understanding of our features."
      ],
      "metadata": {
        "id": "nSXLyr55kExe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Inferring Variable Types ---\")\n",
        "analysis_results = []\n",
        "total_rows = len(df)\n",
        "for col in df.columns:\n",
        "    dtype = df[col].dtype\n",
        "    unique_values = df[col].nunique()\n",
        "\n",
        "    clean_type = 'Other'\n",
        "\n",
        "    if pd.api.types.is_numeric_dtype(df[col]):\n",
        "        if unique_values > total_rows * 0.95:\n",
        "            clean_type = 'Identifier'\n",
        "        else:\n",
        "            clean_type = 'Numerical'\n",
        "    elif pd.api.types.is_object_dtype(df[col]):\n",
        "        if unique_values < 50 or (unique_values / total_rows < 0.02 and unique_values > 1):\n",
        "            clean_type = 'Categorical'\n",
        "        elif unique_values == total_rows:\n",
        "            clean_type = 'Identifier'\n",
        "        else:\n",
        "            clean_type = 'Text/High Cardinality'\n",
        "\n",
        "    if 'date' in col.lower() or col.lower().endswith('_d'):\n",
        "        clean_type = 'Datetime (Potential)'\n",
        "\n",
        "    emoji_map = {\n",
        "        'Identifier': 'ðŸ†”', 'Numerical': 'ðŸ”¢', 'Categorical': 'ðŸ·ï¸',\n",
        "        'Text/High Cardinality': 'ðŸ“„', 'Datetime (Potential)': 'ðŸ“…', 'Other': 'â“'\n",
        "    }\n",
        "    inferred_type_with_emoji = f\"{clean_type} {emoji_map.get(clean_type, '')}\".strip()\n",
        "\n",
        "    analysis_results.append({\n",
        "        'Variable': col, 'Unique Values': unique_values,\n",
        "        'Dtype': dtype, 'Inferred Type': inferred_type_with_emoji,\n",
        "        'Clean Type': clean_type\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(analysis_results)\n",
        "print(results_df.drop(columns=['Clean Type']).to_string(index=False))\n",
        "\n",
        "type_counts = results_df['Clean Type'].value_counts()\n",
        "summary_parts = [f\"{name}({count})\" for name, count in type_counts.items()]\n",
        "summary_string = \"Inferred Types: \" + \", \".join(summary_parts)\n",
        "print(f\"\\n{summary_string}\")"
      ],
      "metadata": {
        "id": "FSXX2H-CkJhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Descriptive Statistics (Numerical)\n",
        "\n",
        "Here we generate a statistical summary (count, mean, standard deviation, quartiles, etc.) for all numerical columns in the dataset."
      ],
      "metadata": {
        "id": "eyQO0VP5kNav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Descriptive Statistics (Numerical) ---\")\n",
        "# Set display options to show all columns and format numbers\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "\n",
        "display(df.describe().T)\n",
        "\n",
        "# Reset display options to default\n",
        "pd.reset_option('display.max_rows')\n",
        "pd.reset_option('display.float_format')"
      ],
      "metadata": {
        "id": "WOvUUIUBkRBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Missing Value Analysis\n",
        "\n",
        "This step identifies which columns are missing the most data. We will display the top 50 columns with the highest counts of missing values."
      ],
      "metadata": {
        "id": "fNVi4H6fkTZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Missing Value Analysis ---\")\n",
        "missing_values = df.isnull().sum()\n",
        "missing_values = missing_values[missing_values > 0].sort_values(ascending=False)\n",
        "\n",
        "if not missing_values.empty:\n",
        "    print(\"Top 50 columns with the most missing values:\")\n",
        "    print(missing_values.head(50))\n",
        "else:\n",
        "    print(\"No missing values found in the dataset. Great!\")"
      ],
      "metadata": {
        "id": "u57xHF2dkWPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Create a Stratified Sample\n",
        "\n",
        "As a final step in this initial inspection, we'll create a small, 100-row stratified sample based on the primary `loan_status` categories. This file is useful for quick spot-checks and manual inspection without needing to load the full dataset."
      ],
      "metadata": {
        "id": "-W_avi61kYqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to create the sample\n",
        "def create_stratified_sample(df_to_sample):\n",
        "    print(\"\\n--- Creating Stratified Sample from DataFrame ---\")\n",
        "    try:\n",
        "        sampling_plan = {\n",
        "            'Fully Paid': int(TOTAL_SAMPLES * 0.40),\n",
        "            'Current': int(TOTAL_SAMPLES * 0.30),\n",
        "            'Charged Off': int(TOTAL_SAMPLES * 0.30)\n",
        "        }\n",
        "        print(\"Sampling plan:\")\n",
        "        for status, count in sampling_plan.items():\n",
        "            print(f\"- {status}: {count} rows\")\n",
        "\n",
        "        sample_list = []\n",
        "        for status, count in sampling_plan.items():\n",
        "            group = df_to_sample[df_to_sample['loan_status'] == status]\n",
        "            if len(group) < count:\n",
        "                print(f\"Warning: Not enough samples for '{status}'. Taking {len(group)} rows.\")\n",
        "                count = len(group)\n",
        "            sample = group.sample(n=count, random_state=42)\n",
        "            sample_list.append(sample)\n",
        "\n",
        "        final_sample = pd.concat(sample_list)\n",
        "        final_sample = final_sample.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "        final_sample.to_csv(SAMPLE_OUTPUT_FILE, index=False)\n",
        "        print(f\"\\nSuccess! A stratified sample of {len(final_sample)} rows has been saved to '{SAMPLE_OUTPUT_FILE}'\")\n",
        "        print(\"You can find this file in the Colab file browser on the left.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during sampling: {e}\")\n",
        "\n",
        "# Call the function using the loaded DataFrame\n",
        "create_stratified_sample(df)"
      ],
      "metadata": {
        "id": "8QMGNXRAkcz4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}