{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Final Model Comparison: ROC Curves and AUC\n",
        "\n",
        "This notebook performs the definitive comparison of our three candidate models:\n",
        "1.  **Logistic Regression** (Baseline)\n",
        "2.  **XGBoost** (Intermediate)\n",
        "3.  **CatBoost** (Champion)\n",
        "\n",
        "To prove which model is best, we will not just look at accuracy. We will plot the **ROC (Receiver Operating Characteristic) Curve**.\n",
        "\n",
        "The ROC curve shows the trade-off between **Recall** (catching bad loans) and **False Positives** (flagging good loans) at every possible probability threshold.\n",
        "* **A perfect model** would bow sharply into the top-left corner.\n",
        "* **The AUC (Area Under the Curve)** gives us a single score to rank them. (1.0 = Perfect, 0.5 = Random Guessing)."
      ],
      "metadata": {
        "id": "LxohguvHba3l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJ_-muQGa3Lf"
      },
      "outputs": [],
      "source": [
        "# Install CatBoost\n",
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import roc_curve, auc, classification_report\n",
        "import os\n",
        "\n",
        "# Create output directory\n",
        "OUTPUT_DIR = \"final_model_comparison\"\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)"
      ],
      "metadata": {
        "id": "wRK_9Pbybjog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Train & Evaluate Logistic Regression\n",
        "We load the **Scaled** dataset (`X_train.csv`) for this model."
      ],
      "metadata": {
        "id": "33Z5JSaVblK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- 1. Evaluating Logistic Regression ---\")\n",
        "try:\n",
        "    # Load the datasets\n",
        "    X_train_lr = pd.read_csv('X_train.csv')\n",
        "    y_train_lr = pd.read_csv('y_train.csv')\n",
        "    X_test_lr = pd.read_csv('X_test.csv')\n",
        "    y_test_lr = pd.read_csv('y_test.csv')\n",
        "\n",
        "    # Drop Non-Numeric Columns\n",
        "    non_numeric_cols = X_train_lr.select_dtypes(exclude=[np.number]).columns\n",
        "\n",
        "    if not non_numeric_cols.empty:\n",
        "        print(f\"   Detected non-numeric columns: {list(non_numeric_cols)}\")\n",
        "        print(\"   Dropping them to prevent errors...\")\n",
        "        X_train_lr = X_train_lr.drop(columns=non_numeric_cols)\n",
        "        X_test_lr = X_test_lr.drop(columns=non_numeric_cols)\n",
        "\n",
        "    # Train\n",
        "    model_lr = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    model_lr.fit(X_train_lr, y_train_lr.values.ravel())\n",
        "\n",
        "    # Get Probabilities (Probability of Default)\n",
        "    probs_lr = model_lr.predict_proba(X_test_lr)[:, 1]\n",
        "\n",
        "    # Calculate ROC metrics\n",
        "    fpr_lr, tpr_lr, _ = roc_curve(y_test_lr, probs_lr)\n",
        "    auc_lr = auc(fpr_lr, tpr_lr)\n",
        "    print(f\"Logistic Regression AUC: {auc_lr:.4f}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Could not find 'X_train.csv'. Please upload the Scaled dataset.\")"
      ],
      "metadata": {
        "id": "Imjv4G9fbmlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Train & Evaluate XGBoost\n",
        "We load the **Tree-Optimized** dataset for this model."
      ],
      "metadata": {
        "id": "cC-qC7LUboi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- 2. Evaluating XGBoost ---\")\n",
        "try:\n",
        "    X_train_tree = pd.read_csv('X_train_tree.csv')\n",
        "    y_train_tree = pd.read_csv('y_train_tree.csv')\n",
        "    X_test_tree = pd.read_csv('X_test_tree.csv')\n",
        "    y_test_tree = pd.read_csv('y_test_tree.csv')\n",
        "\n",
        "    # Train\n",
        "    model_xgb = xgb.XGBClassifier(n_estimators=100, random_state=42, n_jobs=-1, eval_metric='logloss', use_label_encoder=False)\n",
        "    model_xgb.fit(X_train_tree, y_train_tree.values.ravel())\n",
        "\n",
        "    # Get Probabilities\n",
        "    probs_xgb = model_xgb.predict_proba(X_test_tree)[:, 1]\n",
        "\n",
        "    # Calculate ROC metrics\n",
        "    fpr_xgb, tpr_xgb, _ = roc_curve(y_test_tree, probs_xgb)\n",
        "    auc_xgb = auc(fpr_xgb, tpr_xgb)\n",
        "    print(f\"XGBoost AUC: {auc_xgb:.4f}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Could not find 'X_train_tree.csv'. Please upload the Tree dataset.\")"
      ],
      "metadata": {
        "id": "K_PwTYQVbqde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Train & Evaluate CatBoost\n",
        "We load the **Full Feature** dataset for this model."
      ],
      "metadata": {
        "id": "QHW0iw5ybr5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- 3. Evaluating CatBoost ---\")\n",
        "try:\n",
        "    X_train_full = pd.read_csv('X_train_tree_full.csv')\n",
        "    y_train_full = pd.read_csv('y_train_tree_full.csv')\n",
        "    X_test_full = pd.read_csv('X_test_tree_full.csv')\n",
        "    y_test_full = pd.read_csv('y_test_tree_full.csv')\n",
        "\n",
        "    # FIX: Convert categorical columns to integers for CatBoost\n",
        "    cat_cols = ['term', 'grade', 'sub_grade', 'home_ownership', 'verification_status', 'purpose']\n",
        "    for col in cat_cols:\n",
        "        if col in X_train_full.columns:\n",
        "            X_train_full[col] = X_train_full[col].astype(int)\n",
        "            X_test_full[col] = X_test_full[col].astype(int)\n",
        "\n",
        "    # Get indices for CatBoost\n",
        "    cat_indices = [X_train_full.columns.get_loc(c) for c in cat_cols if c in X_train_full.columns]\n",
        "\n",
        "    # Train\n",
        "    model_cb = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, verbose=0, random_seed=42)\n",
        "    model_cb.fit(X_train_full, y_train_full.values.ravel(), cat_features=cat_indices)\n",
        "\n",
        "    # Get Probabilities\n",
        "    probs_cb = model_cb.predict_proba(X_test_full)[:, 1]\n",
        "\n",
        "    # Calculate ROC metrics\n",
        "    fpr_cb, tpr_cb, _ = roc_curve(y_test_full, probs_cb)\n",
        "    auc_cb = auc(fpr_cb, tpr_cb)\n",
        "    print(f\"CatBoost AUC: {auc_cb:.4f}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Could not find 'X_train_tree_full.csv'. Please upload the Full dataset.\")"
      ],
      "metadata": {
        "id": "49DeKBXsbtfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Combined ROC Plot\n",
        "\n",
        "We will plot all three curves on a single chart."
      ],
      "metadata": {
        "id": "MlPA46D7bvbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Plot Logistic Regression\n",
        "plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {auc_lr:.3f})', color='blue', linestyle='--')\n",
        "\n",
        "# Plot XGBoost\n",
        "plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC = {auc_xgb:.3f})', color='orange')\n",
        "\n",
        "# Plot CatBoost\n",
        "plt.plot(fpr_cb, tpr_cb, label=f'CatBoost (AUC = {auc_cb:.3f})', color='green', linewidth=3)\n",
        "\n",
        "# Plot Random Guess Line\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess (AUC = 0.500)')\n",
        "\n",
        "# Formatting\n",
        "plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=12)\n",
        "plt.ylabel('True Positive Rate (Recall)', fontsize=12)\n",
        "plt.title('Final Model Comparison: ROC Curves', fontsize=16)\n",
        "plt.legend(loc='lower right', fontsize=12)\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "# Save\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"final_roc_comparison.png\"))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TR6ScJZVbwui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Performance Comparison Table\n",
        "\n",
        "Finally, we generate a clean DataFrame summarizing the key metrics for performance evaluation."
      ],
      "metadata": {
        "id": "tSXrU2MObyNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "# Helper function to get metrics\n",
        "def get_metrics(y_true, y_pred, probs):\n",
        "    return {\n",
        "        'Accuracy': accuracy_score(y_true, y_pred),\n",
        "        'AUC': roc_auc_score(y_true, probs),\n",
        "        # Extract Class 1 (Default) metrics\n",
        "        'Precision (Default)': classification_report(y_true, y_pred, output_dict=True)['1']['precision'],\n",
        "        'Recall (Default)': classification_report(y_true, y_pred, output_dict=True)['1']['recall'],\n",
        "        'F1 (Default)': classification_report(y_true, y_pred, output_dict=True)['1']['f1-score']\n",
        "    }\n",
        "\n",
        "# Gather all metrics\n",
        "metrics = []\n",
        "metrics.append({'Model': 'Logistic Regression', **get_metrics(y_test_lr, model_lr.predict(X_test_lr), probs_lr)})\n",
        "metrics.append({'Model': 'XGBoost', **get_metrics(y_test_tree, model_xgb.predict(X_test_tree), probs_xgb)})\n",
        "metrics.append({'Model': 'CatBoost', **get_metrics(y_test_full, model_cb.predict(X_test_full), probs_cb)})\n",
        "\n",
        "# Create DataFrame\n",
        "leaderboard = pd.DataFrame(metrics).set_index('Model')\n",
        "leaderboard = leaderboard.sort_values(by='AUC', ascending=False)\n",
        "\n",
        "# Display and Save\n",
        "display(leaderboard)\n",
        "leaderboard.to_csv(os.path.join(OUTPUT_DIR, \"final_model_leaderboard.csv\"))"
      ],
      "metadata": {
        "id": "ww_E_O9QbzxC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}