{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 5.1. Data Preparation for Logistic Regression\n",
        "\n",
        "This notebook prepares the `lc_loans_master_sample.csv` file for a Logistic Regression model.\n",
        "\n",
        "The key steps are:\n",
        "1.  **Feature Selection:** Remove redundant/collinear features (like `grade`, `int_rate`).\n",
        "2.  **Feature Engineering:** Create a `loan_to_income_ratio` feature.\n",
        "3.  **Train-Test Split:** Split the data *before* any transformations to prevent data leakage.\n",
        "4.  **Encoding:** Convert all categorical features (`sub_grade`, `term`, `purpose`, etc.) into a numeric format.\n",
        "5.  **Scaling:** Apply `StandardScaler` to all numerical features, which is essential for Logistic Regression.\n",
        "6.  **Save Output:** Save the final, model-ready `X_train`, `X_test`, `y_train`, and `y_test` files."
      ],
      "metadata": {
        "id": "3dnq-HArBWt0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIzvMGhABH2Y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration\n",
        "Define the input file (our master sample)."
      ],
      "metadata": {
        "id": "G8EOCrixBdxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "INPUT_FILE = 'lc_loans_master_sample.csv'"
      ],
      "metadata": {
        "id": "AUtZZoe2BfmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Load and Prepare Data\n",
        "Load the master sample, create our new engineered feature, and drop the redundant columns to avoid multicollinearity."
      ],
      "metadata": {
        "id": "zU10Nk0_Bjek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload 'lc_loans_master_sample.csv' to Colab\n",
        "try:\n",
        "    df = pd.read_csv(INPUT_FILE)\n",
        "    print(f\"Loaded master sample with shape: {df.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{INPUT_FILE}' was not found.\")\n",
        "    print(\"Please upload the file to this Colab session.\")\n",
        "\n",
        "# 1. Feature Engineering\n",
        "# Add 1 to annual_inc to prevent any divide-by-zero errors\n",
        "df['loan_to_income_ratio'] = df['loan_amnt'] / (df['annual_inc'] + 1)\n",
        "print(\"Created 'loan_to_income_ratio' feature.\")\n",
        "\n",
        "# 2. Feature Selection\n",
        "# Drop redundant/collinear features\n",
        "features_to_drop = [\n",
        "    'grade',       # Redundant with sub_grade\n",
        "    'int_rate',    # Redundant with sub_grade\n",
        "    'installment'  # Redundant with loan_amnt and term\n",
        "\n",
        "    # Drop these unhandled text columns\n",
        "    'pymnt_plan',\n",
        "    'initial_list_status',\n",
        "    'application_type',\n",
        "    'hardship_flag',\n",
        "    'disbursement_method',\n",
        "    'debt_settlement_flag'\n",
        "]\n",
        "df = df.drop(columns=features_to_drop, errors='ignore')\n",
        "print(f\"Dropped {len(features_to_drop)} redundant features.\")"
      ],
      "metadata": {
        "id": "201KJdkRBkQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Define Features (X) and Target (y)\n",
        "Separate the data into `X` (our features) and `y` (what we want to predict)."
      ],
      "metadata": {
        "id": "9bbIpd5sBnIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['target']\n",
        "X = df.drop(columns='target')\n",
        "\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")"
      ],
      "metadata": {
        "id": "t3R_oMDhBosY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Train-Test Split\n",
        "We split the data **before** any encoding or scaling. This is a critical step to prevent \"data leakage,\" which would make our model's test results seem better than they actually are.\n",
        "\n",
        "We will use `stratify=y` to ensure our 50/50 balance is preserved in both the training and testing sets."
      ],
      "metadata": {
        "id": "8uZ2SYb8BrP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,     # 20% for testing, 80% for training\n",
        "    random_state=42,   # Ensures the split is reproducible\n",
        "    stratify=y         # Keeps the 50/50 balance in both sets\n",
        ")\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "BCVUbRfgBs9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Feature Encoding\n",
        "Now we convert our remaining text-based columns into numbers."
      ],
      "metadata": {
        "id": "hsQQE5tKBuX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Starting Feature Encoding ---\")\n",
        "\n",
        "# --- 4.1 Binary Mapping for 'term' ---\n",
        "term_map = {36: 0, 60: 1}\n",
        "X_train['term'] = X_train['term'].map(term_map)\n",
        "X_test['term'] = X_test['term'].map(term_map)\n",
        "print(\"Mapped 'term' to 0 (36mo) and 1 (60mo).\")\n",
        "\n",
        "# --- 4.2 Label Encoding for 'sub_grade' ---\n",
        "# Create an ordered list of all 35 sub-grades from A1 to G5\n",
        "all_sub_grades = sorted(X_train['sub_grade'].unique())\n",
        "# Create the mapping dictionary\n",
        "sub_grade_map = {grade: i for i, grade in enumerate(all_sub_grades)}\n",
        "\n",
        "X_train['sub_grade'] = X_train['sub_grade'].map(sub_grade_map)\n",
        "X_test['sub_grade'] = X_test['sub_grade'].map(sub_grade_map)\n",
        "print(\"Mapped 'sub_grade' to ordinal integers (0-34).\")\n",
        "\n",
        "# --- 4.3 One-Hot Encoding for Nominal Features ---\n",
        "nominal_cols = ['home_ownership', 'purpose', 'verification_status']\n",
        "\n",
        "# Initialize the OneHotEncoder\n",
        "# handle_unknown='ignore' tells it to ignore any new categories it might see in the test set\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "\n",
        "# Fit and transform the training data\n",
        "ohe_train = ohe.fit_transform(X_train[nominal_cols])\n",
        "# Only transform the test data (using the fit from training)\n",
        "ohe_test = ohe.transform(X_test[nominal_cols])\n",
        "\n",
        "# Create DataFrames from the OHE arrays, using the feature names\n",
        "ohe_train_df = pd.DataFrame(ohe_train, columns=ohe.get_feature_names_out(), index=X_train.index)\n",
        "ohe_test_df = pd.DataFrame(ohe_test, columns=ohe.get_feature_names_out(), index=X_test.index)\n",
        "\n",
        "# Drop the original text columns and add the new one-hot columns\n",
        "X_train = pd.concat([X_train.drop(columns=nominal_cols), ohe_train_df], axis=1)\n",
        "X_test = pd.concat([X_test.drop(columns=nominal_cols), ohe_test_df], axis=1)\n",
        "\n",
        "print(f\"One-hot encoded '{nominal_cols}'. New X_train shape: {X_train.shape}\")"
      ],
      "metadata": {
        "id": "hXQnGMA_BwAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Feature Scaling\n",
        "Lastly, we will scale all non-binary numerical features to have a mean of 0 and a standard deviation of 1."
      ],
      "metadata": {
        "id": "xYcqZjvkBy86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Starting Feature Scaling ---\")\n",
        "\n",
        "# Identify all columns that are NOT one-hot encoded\n",
        "ohe_cols = ohe.get_feature_names_out()\n",
        "cols_to_scale = [col for col in X_train.columns if col not in ohe_cols]\n",
        "\n",
        "# Identify non-numeric columns that are still in cols_to_scale\n",
        "non_numeric_cols = X_train[cols_to_scale].select_dtypes(exclude=np.number).columns.tolist()\n",
        "print(f\"Identified non-numeric columns to exclude from scaling: {non_numeric_cols}\")\n",
        "\n",
        "# Exclude the non-numeric columns from cols_to_scale\n",
        "cols_to_scale = [col for col in cols_to_scale if col not in non_numeric_cols]\n",
        "print(f\"Scaling the following columns: {cols_to_scale}\")\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train[cols_to_scale] = scaler.fit_transform(X_train[cols_to_scale])\n",
        "# Only transform the test data (using the scaler fit from the training data)\n",
        "X_test[cols_to_scale] = scaler.transform(X_test[cols_to_scale])\n",
        "\n",
        "print(\"Applied StandardScaler to all numerical and ordinal features.\")\n",
        "print(\"\\n--- Data Preparation Complete ---\")"
      ],
      "metadata": {
        "id": "u5vE_CkgB0nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Save the Prepared Datasets\n",
        "The data is now fully prepared for modeling. We'll export the four DataFrames to new CSV files."
      ],
      "metadata": {
        "id": "xAxU9VFkB2q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the final, processed data\n",
        "X_train.to_csv('X_train.csv', index=False)\n",
        "y_train.to_csv('y_train.csv', index=False)\n",
        "X_test.to_csv('X_test.csv', index=False)\n",
        "y_test.to_csv('y_test.csv', index=False)\n",
        "\n",
        "print(\"Saved X_train, y_train, X_test, and y_test to CSV files.\")\n",
        "\n",
        "print(\"\\n--- Final X_train head: ---\")\n",
        "display(X_train.head())\n",
        "\n",
        "print(\"\\n--- Final X_train info: ---\")\n",
        "X_train.info()"
      ],
      "metadata": {
        "id": "4BV3Jv_tB4pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We verify if the encoding and scaling was performed as expected."
      ],
      "metadata": {
        "id": "jFTIb8s7txDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the file you want to check\n",
        "X_train_check = pd.read_csv('X_train.csv')\n",
        "\n",
        "# Get the summary statistics\n",
        "summary_stats_transposed = X_train_check.describe().round(3).T\n",
        "\n",
        "# Display the summary\n",
        "print(\"--- Summary Statistics for X_train (Transposed, All Variables) ---\")\n",
        "display(summary_stats_transposed)\n",
        "\n",
        "# Reset the display option back to default\n",
        "pd.reset_option('display.max_rows')"
      ],
      "metadata": {
        "id": "nm00GxrJm7nU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}